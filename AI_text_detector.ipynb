{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFID TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Load your dataset\n",
    "# dataset = pd.read_csv(\"Training_Essay_Data.csv\")\n",
    "\n",
    "# # Split the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     dataset['text'], dataset['generated'].astype(int), test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Create a pipeline with TfidfVectorizer and Multinomial Naive Bayes\n",
    "# text_clf = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer()),\n",
    "#     ('clf', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# # Define parameters for Grid Search\n",
    "# param_grid = {\n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'clf__alpha': [0.5, 1.0],\n",
    "# }\n",
    "\n",
    "# # Perform Grid Search for hyperparameter tuning\n",
    "# grid_search = GridSearchCV(text_clf, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best model from Grid Search\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# accuracy = best_model.score(X_test, y_test)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to prompt a choice to input text manually or via .doc or .txt (File Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_text(input_text, model):\n",
    "#     # Predict the class\n",
    "#     prediction = model.predict([input_text])\n",
    "\n",
    "#     # Output the result\n",
    "#     if prediction[0] == 0:\n",
    "#         return \"Human-generated\"\n",
    "#     else:\n",
    "#         return \"AI-generated\"\n",
    "\n",
    "# # Function to read text from a file\n",
    "# def read_text_from_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Prompt user to choose an option\n",
    "# print(\"Choose an option:\")\n",
    "# print(\"1. Enter multiline text manually\")\n",
    "# print(\"2. Upload a doc or text file\")\n",
    "\n",
    "# choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "# if choice == '1':\n",
    "#     user_input_lines = []\n",
    "#     print(\"Enter multiline text. Press Enter twice to finish.\")\n",
    "#     while True:\n",
    "#         line = input()\n",
    "#         if not line:\n",
    "#             break\n",
    "#         user_input_lines.append(line)\n",
    "#     user_input = '\\n'.join(user_input_lines)\n",
    "    \n",
    "#     result = classify_text(user_input, best_model)\n",
    "\n",
    "# elif choice == '2':\n",
    "#     # Prompt user to enter the file path\n",
    "#     file_path = input(\"Enter the path of the text or doc file: \")\n",
    "\n",
    "#     # Check if the file is a doc file and read its content\n",
    "#     if file_path.endswith('.docx'):\n",
    "#         try:\n",
    "#             doc = Document(file_path)\n",
    "#             doc_text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "#             result = classify_text(doc_text, best_model)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading doc file: {e}\")\n",
    "#             result = None\n",
    "#     else:\n",
    "#         # Read text from the file\n",
    "#         input_text = read_text_from_file(file_path)\n",
    "\n",
    "#         # Classify the text\n",
    "#         if input_text:\n",
    "#             result = classify_text(input_text, best_model)\n",
    "#         else:\n",
    "#             result = None\n",
    "# else:\n",
    "#     print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "# # Display the result\n",
    "# if result is not None:\n",
    "#     print(f\"The input text is classified as: {result}\")\n",
    "# else:\n",
    "#     print(\"Classification failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe EMBEDDING TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file_path = \"glove.6B.100d.txt\"  # Adjust the path based on your downloaded file\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"train_essays.csv\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset['text'], dataset['generated'].astype(int), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "max_words = 10000  # Choose an appropriate value\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAD SEQUENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 100  # Choose an appropriate value\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE EMBEDDING MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100  # Use the same dimension as your GloVe file\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 12:58:03.688944: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 3ms/step - loss: 0.8243 - accuracy: 0.9165 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 5/10\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 6/10\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 7/10\n",
      "35/35 [==============================] - 0s 967us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 0s 934us/step - loss: 0.0280 - accuracy: 0.9982 - val_loss: 0.0559 - val_accuracy: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17bd98510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# Add BatchNormalization after Dense layers\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Compile the model (you may need to adjust the loss function and metrics based on your problem)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1378, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1: 3\n",
      "Count of 0: 1375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'label' is the name of the label column in your DataFrame\n",
    "# Replace 'your_dataframe.csv' with the actual file path or DataFrame variable\n",
    "dataset = pd.read_csv(\"train_essays.csv\")\n",
    "\n",
    "# Count occurrences of 1 and 0 in the 'label' column\n",
    "label_counts = dataset['generated'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Count of 1:\", label_counts.get(1, 0))  # 0 if there are no occurrences of 1\n",
    "print(\"Count of 0:\", label_counts.get(0, 0))  # 0 if there are no occurrences of 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to prompt a choice to input text manually or via .doc or .txt (File Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_text(input_text, model, tokenizer, max_sequence_length):\n",
    "#     # Tokenize and pad the input text\n",
    "#     input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "#     input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "#     # Predict the class\n",
    "#     prediction = model.predict(input_padded)\n",
    "\n",
    "#     # Output the result\n",
    "#     if prediction[0] < 0.5:\n",
    "#         return \"Human-generated\"\n",
    "#     else:\n",
    "#         return \"AI-generated\"\n",
    "\n",
    "# # Function to read text from a file\n",
    "# def read_text_from_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Prompt user to choose an option\n",
    "# print(\"Choose an option:\")\n",
    "# print(\"1. Enter multiline text manually\")\n",
    "# print(\"2. Upload a doc or text file\")\n",
    "\n",
    "# choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "# if choice == '1':\n",
    "#     user_input_lines = []\n",
    "#     print(\"Enter multiline text. Press Enter twice to finish.\")\n",
    "#     while True:\n",
    "#         line = input()\n",
    "#         if not line:\n",
    "#             break\n",
    "#         user_input_lines.append(line)\n",
    "#     user_input = '\\n'.join(user_input_lines)\n",
    "    \n",
    "#     result = classify_text(user_input, model= model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
    "\n",
    "# elif choice == '2':\n",
    "#     # Prompt user to enter the file path\n",
    "#     file_path = input(\"Enter the path of the text or doc file: \")\n",
    "\n",
    "#     # Check if the file is a doc file and read its content\n",
    "#     if file_path.endswith('.docx'):\n",
    "#         try:\n",
    "#             doc = Document(file_path)\n",
    "#             doc_text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "#             result = classify_text(doc_text, model= model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading doc file: {e}\")\n",
    "#             result = None\n",
    "#     else:\n",
    "#         # Read text from the file\n",
    "#         input_text = read_text_from_file(file_path)\n",
    "\n",
    "#         # Classify the text\n",
    "#         if input_text:\n",
    "#             result = classify_text(input_text, model= model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
    "#         else:\n",
    "#             result = None\n",
    "# else:\n",
    "#     print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "# # Display the result\n",
    "# if result is not None:\n",
    "#     print(f\"The input text is classified as: {result}\")\n",
    "# else:\n",
    "#     print(\"Classification failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext, messagebox, filedialog\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from docx import Document\n",
    "\n",
    "# def classify_text(input_text, model, tokenizer, max_sequence_length):\n",
    "#     # Tokenize and pad the input text\n",
    "#     input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "#     input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "#     # Predict the class\n",
    "#     prediction = model.predict(input_padded)\n",
    "\n",
    "#     # Output the result\n",
    "#     if prediction[0] < 0.5:\n",
    "#         return \"Human-generated\"\n",
    "#     else:\n",
    "#         return \"AI-generated\"\n",
    "\n",
    "# def read_text_from_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading file: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def calculate_classification():\n",
    "#     user_input = text_entry.get(\"1.0\", \"end-1c\")\n",
    "#     if user_input.strip() == \"\":\n",
    "#         messagebox.showinfo(\"Error\", \"Please enter text.\")\n",
    "#         return\n",
    "\n",
    "#     result = classify_text(user_input, model=model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
    "#     result_label.config(text=f\"The input text is classified as: {result}\")\n",
    "\n",
    "# def open_file_dialog():\n",
    "#     file_path = filedialog.askopenfilename(title=\"Select a file\", filetypes=[(\"Text files\", \"*.txt\"), (\"Word files\", \"*.docx\")])\n",
    "#     if file_path:\n",
    "#         if file_path.endswith('.docx'):\n",
    "#             try:\n",
    "#                 doc = Document(file_path)\n",
    "#                 doc_text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "#                 text_entry.delete(\"1.0\", \"end\")\n",
    "#                 text_entry.insert(\"1.0\", doc_text)\n",
    "#             except Exception as e:\n",
    "#                 messagebox.showinfo(\"Error\", f\"Error reading doc file: {e}\")\n",
    "#         else:\n",
    "#             input_text = read_text_from_file(file_path)\n",
    "#             if input_text:\n",
    "#                 text_entry.delete(\"1.0\", \"end\")\n",
    "#                 text_entry.insert(\"1.0\", input_text)\n",
    "#             else:\n",
    "#                 messagebox.showinfo(\"Error\", \"Error reading file.\")\n",
    "\n",
    "# # Create the main window\n",
    "# window = tk.Tk()\n",
    "# window.title(\"Text Classification GUI\")\n",
    "\n",
    "# # Create text entry\n",
    "# text_entry = scrolledtext.ScrolledText(window, width=40, height=10, wrap=tk.WORD)\n",
    "# text_entry.pack(padx=10, pady=10)\n",
    "\n",
    "# # Create buttons\n",
    "# calculate_button = tk.Button(window, text=\"Calculate\", command=calculate_classification)\n",
    "# calculate_button.pack(pady=5)\n",
    "\n",
    "# open_file_button = tk.Button(window, text=\"Open File\", command=open_file_dialog)\n",
    "# open_file_button.pack(pady=5)\n",
    "\n",
    "# # Create result label\n",
    "# result_label = tk.Label(window, text=\"\")\n",
    "# result_label.pack(pady=5)\n",
    "\n",
    "# # Run the GUI\n",
    "# window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saksham2510/anaconda3/envs/nlp_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QLabel, QPushButton, QTextEdit, QFileDialog, QHBoxLayout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "def classify_text(input_text, model, tokenizer, max_sequence_length):\n",
    "    # Tokenize and pad the input text\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = model.predict(input_padded)\n",
    "\n",
    "    # Output the result\n",
    "    if prediction[0] < 0.5:\n",
    "        return \"Human-generated\"\n",
    "    else:\n",
    "        return \"AI-generated\"\n",
    "\n",
    "class TextClassificationApp(QWidget):\n",
    "    def __init__(self, model, tokenizer, max_sequence_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        self.init_ui()\n",
    "\n",
    "    def init_ui(self):\n",
    "        layout = QVBoxLayout()\n",
    "\n",
    "        # Text editor\n",
    "        self.text_edit = QTextEdit()\n",
    "        layout.addWidget(self.text_edit)\n",
    "\n",
    "        # Buttons layout\n",
    "        buttons_layout = QHBoxLayout()\n",
    "\n",
    "        # Calculate button\n",
    "        calculate_button = QPushButton('Calculate')\n",
    "        calculate_button.clicked.connect(self.calculate_button_clicked)\n",
    "        buttons_layout.addWidget(calculate_button)\n",
    "\n",
    "        # Upload button\n",
    "        upload_button = QPushButton('Upload File')\n",
    "        upload_button.clicked.connect(self.upload_button_clicked)\n",
    "        buttons_layout.addWidget(upload_button)\n",
    "\n",
    "        # Close button\n",
    "        close_button = QPushButton('Close')\n",
    "        close_button.clicked.connect(self.close_application)\n",
    "        buttons_layout.addWidget(close_button)\n",
    "\n",
    "        layout.addLayout(buttons_layout)\n",
    "\n",
    "        # Result label\n",
    "        self.result_label = QLabel('')\n",
    "        layout.addWidget(self.result_label)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "        self.setWindowTitle('Text Classification App')\n",
    "        self.setGeometry(100, 100, 600, 400)\n",
    "\n",
    "    def calculate_button_clicked(self):\n",
    "        input_text = self.text_edit.toPlainText()\n",
    "        if input_text:\n",
    "            result = classify_text(input_text, self.model, self.tokenizer, self.max_sequence_length)\n",
    "            self.result_label.setText(f'The input text is classified as: {result}')\n",
    "        else:\n",
    "            self.result_label.setText('Please enter text.')\n",
    "\n",
    "    def upload_button_clicked(self):\n",
    "        file_dialog = QFileDialog()\n",
    "        file_path, _ = file_dialog.getOpenFileName(self, 'Upload Text File', '', 'Text Files (*.txt);;All Files (*)')\n",
    "        if file_path:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()\n",
    "                self.text_edit.setPlainText(file_content)\n",
    "            self.result_label.clear()\n",
    "\n",
    "    def close_application(self):\n",
    "        # Explicitly release resources or perform cleanup if needed\n",
    "        self.close()\n",
    "\n",
    "# Rest of your code remains unchanged\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace 'model', 'tokenizer', and 'max_sequence_length' with your actual model and tokenizer\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    max_sequence_length = 100  # Replace with your actual value\n",
    "\n",
    "    app = QApplication(sys.argv)\n",
    "    window = TextClassificationApp(model, tokenizer, max_sequence_length)\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
